[TOC]

## Intro

- Preprocessing 
- Feature Extraction
- Classification

## ğŸ’– Bayesian

### Theory

æœ€å°åŒ–é£é™©ï¼š
$$
R(c_i|x)=\sum_{j=1}^N\lambda_{ij}P(c_j|x)
$$
è¡¨ç¤ºï¼ŒçŒœæµ‹xæ˜¯$c_i$ ç±»ï¼Œè·å¾—çš„æŸå¤±ã€‚$\lambda_{ij}$æŠŠjç±»å½“æˆiç±»é€ æˆçš„æŸå¤±ã€‚
$$
P(c_j|x)=\frac{p(c_j, x)}{p(x)}=\frac{p(x|c_j)p(c_j)}{\sum p(x|c_i)p(c_i)}
$$
ç›®æ ‡ï¼Œå¯»æ‰¾åˆ¤å®šå‡†åˆ™hï¼š
$$
R(h)=\mathbb E_x[R(h(x)|x)]
$$

æ­£æ€åˆ†å¸ƒè´å¶æ–¯å†³ç­–ï¼š

- æŠŠæ¯ä¸ªç±»çš„åˆ†å¸ƒçœ‹åšæ­£æ€åˆ†å¸ƒï¼›

1. $\Sigma_i=\sigma^2I$ï¼Œå„ç±»ç‰¹å¾ç‹¬ç«‹ï¼Œæ–¹å·®ç›¸ç­‰ï¼›

$$
g_i(x)=-\frac{1}{2\sigma^2}||x-\mu_i||^2
$$

2. $\Sigma_i=\Sigma$ï¼Œå„ç±»åæ–¹å·®çŸ©é˜µç›¸åŒï¼š

çº¿æ€§åˆ¤åˆ«
$$
\begin{aligned}
g_i(x)&=-\frac{1}{2}(x-\mu_i)^T\Sigma^{-1}(x-\mu_i)+\ln p(w_i)\\
&=\Sigma^{-1}\mu_i x+\left(-\frac{1}{2}\mu_i^T\Sigma^{-1}\mu_i+\ln p(w_i)\right)\\
&= w_i^Tx+b_i
\end{aligned}
$$

3. å„ç±»åæ–¹å·®ä¸åŒï¼›

### Exercise

![](https://i.loli.net/2019/12/17/5yDJZIgvUftWRaB.png)
$$
p(w_1|x)=\frac{p(w_1, x)}{p(x)}=\frac{0.2*0.9}{0.6}=0.3\\
p(w_2|x)=\frac{p(w_2, x)}{p(x)}=\frac{0.1*0.4}{0.6}=0.07
$$
æ‰€ä»¥åº”è¯¥è®¤ä¸ºæ˜¯ç¬¬ä¸€ç±»ï¼›

![](https://i.loli.net/2019/12/17/JAq7RkQ25cSlZT6.png)
$$
R(c_1|x)=0*0.3+6*0.07=0.42\\
R(c_2|x)=1*0.3=0.3
$$
æ‰€ä»¥$c_1$é£é™©å¤§ï¼Œé€‰2ï¼›

![](https://i.loli.net/2019/12/17/9fJ6ONguK4rx8T2.png)

$$
p(w_1)=0.5\%\\
p(w_2)=99.5\%\\
p(x|w_2)=1\%\\
p(x|w_1)=95\%\\
p(w_1|x)=\frac{p(w_1, x)}{p(x)}=\frac{p(x|w_1)p(w_1)}{p(x)}=0.005\times 0.95\\
p(w_2|x)=\frac{p(w_2,x)}{p(x)}=\frac{p(x|w_2)p(w_2)}{p(x)}=0.995\times 0.01
$$

## Maximum Likehood & Bayesian Parameter Estimation

$$
P(c_j|x)=\frac{p(c_j, x)}{p(x)}=\frac{p(x|c_j)p(c_j)}{\sum p(x|c_i)p(c_i)}
$$

åˆ†æ¯ä¸ç”¨ç®¡ï¼Œåˆ†å­è¦æ±‚ï¼š

1. $p(x|c_j)$ æœ‰äº†æ•°æ®é›†ä¹‹åï¼Œèµ°å‚æ•°ä¼°è®¡ï¼Œæ±‚å‡ºã€Œæ¦‚ç‡åˆ†å¸ƒå‡½æ•°ã€ï¼›
2. $p(c_j)$ æ•°æ®é›†ç±»åˆ«æ¯”ä¾‹ï¼Œæœ‰æ•°æ®é›†ç›´æ¥æ±‚ï¼›

å‚æ•°ä¼°è®¡ä¸€èˆ¬2æ¡è·¯ï¼š

- MLE -> æ±‚å‚æ•°å€¼
- Baysian -> æ±‚å‚æ•°åˆ†å¸ƒ

### ğŸ’– MLE å‚æ•°ä¼°è®¡

> ç‰¹ç‚¹ï¼š
>
> - å‚æ•°æ˜¯å›ºå®šå€¼ï¼Œä½†æˆ‘ä»¬ä¸æ™“å¾—ï¼›
> - æœ€ä½³å‚æ•°å¯ä»¥é€šè¿‡ä¼˜åŒ–ä¼¼ç„¶å¾—åˆ°ï¼›

è´å¶æ–¯ç†è®ºè¦æ±‚æˆ‘ä»¬æ±‚$P(x)=\sum_i P(x|c_i)P(c_i) $ï¼›æ‰€ä»¥æˆ‘ä»¬è¦æ±‚$P(x)$ï¼ˆä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼‰ã€‚

ç„¶åæœºå™¨å­¦ä¹ å˜›ï¼Œlearn from dataï¼Œæˆ‘ä»¬éœ€è¦æœ‰dataï¼›

é¦–å…ˆæˆ‘ä»¬è¦å‡è®¾cç±»ä¸‹çš„çœŸå®æ•°(å¯ç”¨æ•°æ®é›†$D_c$è¡¨ç¤º)åˆ†å¸ƒç¬¦åˆæŸç§æ¦‚ç‡åˆ†å¸ƒ$P$ï¼Œç„¶åå°†$P$å¸¦å…¥åˆ°$P(x|\theta_c)$ï¼Œä½†æ˜¯è¿™ä¸ªæ¦‚ç‡åˆ†å¸ƒå¯èƒ½æœ‰ä¸€äº›å‚æ•°ï¼Œé‚£å°±æ˜¯$\theta_c$ã€‚å‡è®¾è¿™ä¸ªå€¼å’Œå‚æ•°å‘é‡$\theta_c$å”¯ä¸€æœ‰å…³ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±æ˜¯<u>åˆ©ç”¨è®­ç»ƒé›†è®­ç»ƒæ±‚å¾—$\theta_c$ã€‚</u>

å‡è®¾è¿™äº›æ ·æœ¬æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ï¼Œé‚£ä¹ˆ**ä¼¼ç„¶**å°±æ˜¯ï¼š
$$
L(D_c|\theta_c)=\prod_{x\in D_c}P(x|\theta_c)
$$
å¯¹æ•°ä¼¼ç„¶å°±æ˜¯ï¼š
$$
LL(D_c|\theta_c)=\sum_{x\in D_c}\ln P(x|\theta_c)
$$
æˆ‘ä»¬è¦é€šè¿‡è®¾ç½®$\theta_c$ï¼Œä½¿ä¼¼ç„¶å°½å¯èƒ½å¤§ï¼š
$$
\hat\theta_c=\arg\max_{\theta_c}LL(D_c|\theta_c)
$$
ç„¶åæœ€ä¼˜åŒ–ï¼Œæ±‚å¯¼å³å¯è§£ã€‚

> ä¸€ä¸ªé‡ç‚¹å°±æ˜¯ï¼šå¦‚æœä½¿ç”¨MLEï¼Œé‚£ä¹ˆé¢„æµ‹çš„distributionè¦å°½é‡ç¬¦åˆçœŸå®çš„æ•°æ®åˆ†å¸ƒï¼Œå¦åˆ™å‡‰å‡‰ã€‚

#### ä¾‹å­

$$
f(X;\theta) = \theta e^{-\theta x},{\rm if\ x\ge 0, otherwise\ 0}\\
l(\theta)=\sum \ln f(x|\theta)=n\ln \theta - \theta \sum x_i, x_i\ge 0\\
\frac{d l}{d\theta}=\frac{n}{\theta}-\sum x_i, x_i\ge 0\\
$$

æ±‚æœ€ä¼˜è§£å³å¯ã€‚

### Bayesian Estimation

åœ¨BEä¸­ï¼Œå…¶å‚æ•°æ˜¯ä¸€ä¸ªå˜é‡ï¼Œè€Œéå®šå€¼ï¼›
$$
P(c_j|x)=\frac{p(c_j, x)}{p(x)}=\frac{p(x|c_j)p(c_j)}{\sum p(x|c_i)p(c_i)}
$$
æˆ‘ä»¬è¦æ±‚$p(x|c_j)$ï¼Œå³æ±‚å…¶æ¦‚ç‡åˆ†å¸ƒæ¨¡å‹å’Œå‚æ•°$\theta$ ï¼Œè¿™æ˜¯MLEå¹²çš„ï¼Œè€Œè´å¶æ–¯å‚æ•°ä¼°è®¡çš„ç‰¹ç‚¹æ˜¯ä¸æ±‚å‚ï¼Œè€Œæ˜¯æ±‚å‚æ•°çš„åˆ†å¸ƒ$p(\theta)$ ã€‚

å…¶å‚æ•°åˆ†å¸ƒä¸æ•°æ®é›†æœ‰å…³ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†æ˜¯Dçš„æ—¶å€™ï¼Œå‚æ•°çš„åˆ†å¸ƒå°±æ˜¯$p(\theta|D)$ã€‚

å¢é‡å­¦ä¹ ï¼š

ä¸€å¼€å§‹æˆ‘ä»¬æ²¡æœ‰æ ·æœ¬ï¼Œé‚£ä¹ˆå‚æ•°åˆ†å¸ƒç›´æ¥ä¸º$p(\theta) = p(\theta|D^0)$ï¼Œæ¯æ¬¡å¢åŠ ä¸€ä¸ªæ ·æœ¬$x_n$ï¼Œæˆ‘ä»¬æœ‰ï¼š

$$
p(D^n|\theta)=p(x_n|\theta)p(D^{n-1}|\theta)\\
p(\theta|D^n)=\frac{p(x_n|\theta)p(\theta|D^{n-1})}{\int p(x_n|\theta)p(\theta|D^{n-1})d\theta}\sim p(x_n|\theta)p(\theta|D^{n-1})
$$

#### ä¾‹å­

æˆ‘ä»¬ä¸€å¼€å§‹çŸ¥é“æˆ‘ä»¬çš„å‚æ•°wçš„èŒƒå›´æ˜¯`(0, 10]`ï¼Œæˆ‘ä»¬çš„æ•°æ®æ˜¯`D = {4, 7, 2, 8}`ã€‚

åˆ†å¸ƒ:
$$
\mathbf{distribution}\to p(x|w)\sim U(0,w)=1/w\ \ {\rm if}\ \ x\in[0,w],\rm otherwise\ 0
$$

$$
\begin{aligned}
&p(w|D^0)=p(w)=U(0,10)=&1/w&\to w\in (0, 10]\\
&p(w|D^1)\sim p(x_1|w)p(w|D^0)=&1/w^2&\to w\in [4, 10]\\
&p(w|D^n)\sim p(x_n|w)p(w|D^{n-1})=&1/w^{n+1}&\to w\in[\max{D^n},10]
\end{aligned}
$$
åŒä¸Šï¼Œæˆ‘ä»¬è¦æ±‚$p(x|c_j)$ï¼Œå…¶å®è¿˜æ˜¯æ±‚$p(x|D)$ã€‚
$$
\begin{aligned}
&p(\theta|D)\to p(x|D) = \int p(x|\theta)p(\theta|D)d\theta\\
&p(\theta|D)=\frac{p(\theta,D)}{p(D)=\int p(D|\theta)p(\theta)d \theta}\\
&p(D|\theta)=\prod_{k=1}^np(x_k|\theta)
\end{aligned}
$$
æœ€åä¸€è¡Œï¼Œæƒ³çŸ¥é“æ•´ä¸ªæ•°æ®é›†ï¼Œå°±æ˜¯æƒ³çŸ¥é“æ¯ä¸€ä¸ªæ ·æœ¬çš„æ¦‚ç‡çš„è¿ä¹˜ã€‚

### æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨

è¿™é‡Œ$x_i$æ˜¯xçš„ç¬¬iä¸ªå±æ€§å€¼ã€‚å¯ç›´æ¥ç†è§£å…¬å¼ã€‚
$$
P(c|x)=\frac{P(c)P(x|c)}{P(x)}=\frac{P(c)}{P(x)}\prod_{i=1}^dP(x_i|c)
$$

## Linear Regression

å¯¹äºç›¸äº’çº¿æ€§å¯åˆ†çš„cç±»ï¼Œæœ€å¤š$\frac{c(c-1)}{2}$å¯åˆ†ã€‚

### Generalized Linear Discriminant Functions 

<img src="https://i.loli.net/2019/12/22/NqBQyc2fSH7d6Ra.png" style="zoom:50%;" />

Not linearly seperable? Define a  new space:

e.g. `[x1, x2] -> [x1, x2, x1x2]`

### ğŸ’– Fisher Linear Discrimination

- Minimize within-class scatter Sw. å„ç±»åæ–¹å·®çŸ©é˜µå’Œ
- Maximize between-class scatter Sb. å„ç±»å‡å€¼çš„åæ–¹å·®

$$
w^*x+w_0=0\\
w^*=S_w^{-1}(\mu_1-\mu_2)\\
w_0=-w^{*\top}\frac{\sum_i^N \mu_i}{N}\\
S_w=\sum S_i=\sum \left(\sum_{x\in K_i} (x-\mu_i)(x-\mu_i)^\top\right)
$$

$\mu_i$ï¼Œå¯¹äºç¬¬iç±»ï¼Œxçš„å‡å€¼ã€‚

#### ä¾‹å­

<img src="https://i.loli.net/2019/12/22/Lb4BTQOalEieHYf.png" style="zoom:50%;" />
$$
S_w={\rm diag(2,2)}\\
w^*={\rm diag(1/2,1/2)}[0,-2]^\top=[0,-1]^\top\\
w_0=-w^{*\top}\frac{m_1+m_2}{2}=1
$$
(w0æ˜¯ä¸€ä¸ªæ ‡é‡)

### ğŸ’– Perceptron & example

$$
x_1,x_2\to c_1\\
x_3,x_4\to c_2
$$

é’ˆå¯¹äºå¢å¹¿ï¼ˆè´Ÿæ ·æœ¬è¿˜å¾—åå‘ä¸€ä¸‹ï¼‰åçš„å½¢å¼ï¼›

<img src="https://i.loli.net/2019/12/22/L28dD3RmJHiCbMu.png" style="zoom:40%;" />

è¿™é‡Œå°±æ˜¯å¯¹äº†ä¸ç®¡ï¼Œé”™äº†å°±w += cxï¼ˆå› ä¸ºwx+bå¯¹wæ±‚å¯¼å°±æ˜¯xï¼‰

ï¼ˆç­‰äº0ä¸€æ ·è¦æ›´æ–°ï¼Œäººè„‘è®­ç»ƒï¼Œæ²¡åŠæ³•è¿™å°±æ˜¯ä½ åŒæµçš„è¦æ±‚ orzï¼‰

## ğŸ’– Clustering

### Kmeans

#### Steps

1. Data & K
2. Select K centroids randomly
3. For each non-centroids, find the closest centroid, and join its cluster.
4. Refresh the centroids: $c_{i}'={\rm avg}(x),x\in C_i$. 
5. If new centroids are different to the older ones, goto step 3.

#### How to choose a good k

Global variance:
$$
J(D)=\sum_i \sum_j||x_j-\mu_i||^2
$$
æŠŠå„ä¸ªclusterçš„æ–¹å·®ç®—å‡ºæ¥ï¼Œæ±‚å’Œã€‚

è¿™æ—¶å€™kæ˜¯å˜é‡ï¼Œå¤šç®—å‡ ä¸ªkã€‚

ä¸€èˆ¬æ¥è¯´ï¼Œéšç€kå¢å¤§ï¼ŒJ(D)ä¼šä¸‹é™ï¼›æˆ‘ä»¬å°±é€‰éšç€kå¢å¤§æ—¶ï¼ŒJ(D)å‡å°‘æœ€çŒ›ä¸€æ­¥æ—¶å¯¹åº”çš„kã€‚

### Hierarchical Clustering

#### Hierarchical Clustering 1

##### Distance: Single Link

Smallest distance between all possible cross-cluster point pairs in 2 clusters.


$$
d(C_i, C_j)=\min (d(x_i,x_j)),x_i\in C_i, x_j\in C_j
$$

##### Distance: Complete Link

Largest distance ...

##### Distance: Avg

Average distance among all possible cross-cluster point pairs in 2 clusters.

> è®¡ç®—è·ç¦»çš„æ—¶å€™çš„ä¸€èˆ¬æ­¥éª¤ï¼ˆè€å¸ˆè¦æ±‚çš„æ­¥éª¤ï¼‰ï¼š
>
> ![](https://i.loli.net/2019/12/23/Xm9AfG2WoNvuEDL.png)



##### Agglomerative: Bottom-up

> Just like a huffman tree.

Link the closest clusters.

> ä¸€èˆ¬é€‰ç”¨æ¬§å¼è·ç¦»ï¼š
>
> æ¯æ¬¡é€‰æœ€è¿‘çš„ç‚¹å¯¹(A, B)ï¼Œç„¶åmerge ç±»Aå’Œç±»Bã€‚
>
> å¦‚æœmergeåçš„è¿æ¥æ˜¯single linkï¼Œé‚£ä¹ˆmerge(A, B)åçš„è·ç¦»ä¸ºï¼š
> $$
> d_{(A,B)\to C}=\min(d_{A\to C}, d_{B\to C})
> $$



##### Divisive: Top-down

- Initially all objs are in one big cluster.
- Subdivide the cluster into small clusters.

## ğŸ’– PCA

- ä¸­å¿ƒåŒ–ï¼Œæ‰€æœ‰æ•°æ®å‡å»å¹³å‡å€¼ï¼š$Y=X-\bar X$ (m,n)
- åæ–¹å·®çŸ©é˜µï¼š$\Sigma = \frac{YY^T}{n}$ (m, m)ï¼›
- $\Sigma v = \lambda v$
- å°†ç‰¹å¾å€¼ä»å°åˆ°å¤§æ’åº
- å–æœ€å¤§çš„kä¸ªç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼Œæ‹¼æˆ$P$(kÃ—m)
- $Z_{(k\times n)}= PY$

#### How To Choose K

$$
{\rm PoV}=\frac{\sum_j^k \lambda_j}{\sum_i^d \lambda_i}
$$

Typically, stop at PoV > 0.9.

#### ä¸‰é˜¶è¡Œåˆ—å¼

![](https://i.loli.net/2019/12/24/uX2z4mjE9NVwR5L.png)

#### æ±‚é€†çŸ©é˜µ

é«˜æ–¯æ¶ˆå…ƒæ³•
$$
[A|I]\to [I|A^{-1}]
$$


#### ğŸ˜² ç®—ç‰¹å¾å‘é‡

![](https://i.loli.net/2019/12/24/a2skQfmrKYWqDoh.png)

### Class Separability Criterion

![](https://i.loli.net/2019/12/24/YfVkQtWK4OJGs6I.png)

![](https://i.loli.net/2019/12/24/pDM91Q2B5auvHhA.png)

