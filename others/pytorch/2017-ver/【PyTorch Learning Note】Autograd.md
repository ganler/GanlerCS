# ã€PyTorch Learning Noteã€‘Autograd



## Autogradç®€ä»‹

`autograd`æ˜¯`PyTorch`ä¸­æœ€æ ¸å¿ƒçš„ä¸€ä¸ªåŒ…ã€‚å…¶ä¸»è¦æ˜¯é¢å¯¹BPç®—æ³•è€Œè®¾è®¡çš„ã€‚

> â€‹	å¤§å¤šæ•°åå‘ä¼ æ’­è¿‡ç¨‹éœ€è¦æ‰‹åŠ¨å®ç°ã€‚è¿™å¯¹äºåƒçº¿æ€§å›å½’ç­‰è¾ƒä¸ºç®€å•çš„æ¨¡å‹æ¥è¯´ï¼Œè¿˜å¯ä»¥åº”ä»˜ï¼Œä½†å®é™…ä½¿ç”¨ä¸­ç»å¸¸å‡ºç°éå¸¸å¤æ‚çš„ç½‘ç»œç»“æ„ï¼Œæ­¤æ—¶å¦‚æœæ‰‹åŠ¨å®ç°åå‘ä¼ æ’­ï¼Œä¸ä»…è´¹æ—¶è´¹åŠ›ï¼Œè€Œä¸”å®¹æ˜“å‡ºé”™ï¼Œéš¾ä»¥æ£€æŸ¥ã€‚`torch.autograd`å°±æ˜¯ä¸ºæ–¹ä¾¿ç”¨æˆ·ä½¿ç”¨ï¼Œè€Œä¸“é—¨å¼€å‘çš„ä¸€å¥—è‡ªåŠ¨æ±‚å¯¼å¼•æ“ï¼Œå®ƒèƒ½å¤Ÿæ ¹æ®è¾“å…¥å’Œå‰å‘ä¼ æ’­è¿‡ç¨‹è‡ªåŠ¨æ„å»º**è®¡ç®—å›¾**ï¼Œå¹¶æ‰§è¡Œåå‘ä¼ æ’­ã€‚

#### Variable

`autograd`ä¸­çš„æ ¸å¿ƒæ•°æ®ç»“æ„æ˜¯`Variable`ã€‚`Variable`å°è£…äº†`tensor`ï¼Œå¹¶è®°å½•å¯¹`tensor`çš„æ“ä½œè®°å½•ç”¨æ¥æ„å»ºè®¡ç®—å›¾ã€‚`Variable`çš„æ•°æ®ç»“æ„å¦‚å›¾æ‰€ç¤ºï¼Œä¸»è¦åŒ…å«ä¸‰ä¸ªå¾ˆé‡è¦å’Œæœ‰ç”¨çš„**å±æ€§**ï¼š

> - `data`ï¼šä¿å­˜`Variable`æ‰€åŒ…å«çš„`Tensor`ã€‚
>
> - `grad`ï¼šä¿å­˜`data`å¯¹åº”çš„æ¢¯åº¦ï¼Œ`grad`ä¹Ÿæ˜¯variableï¼Œè€Œä¸æ˜¯tensorï¼Œå®ƒä¸`data`å½¢çŠ¶ä¸€è‡´ã€‚
>
> - `grad_fn`ï¼š æŒ‡å‘ä¸€ä¸ª`Function`ï¼Œè®°å½•`Tensor`çš„æ“ä½œå†å²ï¼Œå³å®ƒæ˜¯ä»€ä¹ˆæ“ä½œçš„è¾“å‡ºï¼Œç”¨æ¥æ„å»ºè®¡ç®—å›¾ã€‚å¦‚æœæŸä¸€ä¸ªå˜é‡æ˜¯ç”±ç”¨æˆ·åˆ›å»ºï¼Œåˆ™å®ƒä¸ºå¶å­èŠ‚ç‚¹ï¼ˆæ²¡æœ‰å­èŠ‚ç‚¹çš„èŠ‚ç‚¹ï¼‰ï¼Œå¯¹åº”çš„`grad_fn`ç­‰äº`None`ã€‚
>
>   > ç®€è€Œè¨€ä¹‹ï¼Œå¯ä»¥è®¤ä¸ºæ˜¯å¯¹ä¸Šä¸€ä¸ªæ“ä½œçš„è®°å½•ï¼ˆå¦‚`x.grad_fn`å°±æ˜¯å¯¹åº”çš„æ˜¯(æœ€æ–°çš„é‚£ä¸ª)å¾—åˆ°xçš„é‚£ä¸ªå¼å­ï¼‰ã€‚

`Variable`çš„æ„é€ å‡½æ•°éœ€è¦ä¼ å…¥`Tensor`ï¼ŒåŒæ—¶æœ‰ä¸¤ä¸ªå¯é€‰å‚æ•°ï¼š

> - `requires_grad (bool)`ï¼šæ˜¯å¦è¦å¯¹è¯¥`Variable`æ±‚æ¢¯åº¦ã€‚
> - `volatile (bool)`ï¼šæ„ä¸ºâ€œæŒ¥å‘â€ï¼Œè®¾ç½®ä¸º`True`ï¼Œåˆ™æ„å»ºåœ¨è¯¥variableä¹‹ä¸Šçš„å›¾éƒ½ä¸ä¼šæ±‚å¯¼ï¼Œä¸“ä¸º<u>æ¨ç†é˜¶æ®µ</u>è®¾è®¡ã€‚

â€‹	Variableæä¾›äº†å¤§éƒ¨åˆ†tensoræ”¯æŒçš„å‡½æ•°ï¼Œä½†å…¶ä¸æ”¯æŒéƒ¨åˆ†`inplace`å‡½æ•°ï¼Œå› è¿™äº›å‡½æ•°ä¼šä¿®æ”¹`Tensor`è‡ªèº«ï¼Œè€Œåœ¨BPNNä¸­ï¼Œ`Variable`éœ€è¦ç¼“å­˜åŸæ¥çš„`Tensor`æ¥è®¡ç®—åå‘ä¼ æ’­æ¢¯åº¦ã€‚å¦‚æœæƒ³è¦è®¡ç®—å„ä¸ª`Variable`çš„æ¢¯åº¦ï¼Œåªéœ€è°ƒç”¨æ ¹èŠ‚ç‚¹`Variable`çš„`backward`æ–¹æ³•ï¼Œ`autograd`ä¼šè‡ªåŠ¨æ²¿ç€è®¡ç®—å›¾åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¯ä¸€ä¸ªå¶å­èŠ‚ç‚¹çš„æ¢¯åº¦ã€‚

> `variable.backward(grad_variable=None, retain_graph=None, create_graph-None)`
>
> - **grad_variables**ï¼šå½¢çŠ¶ä¸`variable`ä¸€è‡´ï¼Œå¯¹äº`y.backward()`ï¼Œ`grad_variables`ç›¸å½“äºé“¾å¼æ³•åˆ™${dz \over dx}={dz \over dy} \times {dy \over dx}$ä¸­çš„$\textbf {dz} \over \textbf {dy}$ã€‚`grad_variables`ä¹Ÿå¯ä»¥æ˜¯`Tensor`æˆ–åºåˆ—ã€‚
> - **retain_graph**ï¼šåå‘ä¼ æ’­éœ€è¦ç¼“å­˜ä¸€äº›ä¸­é—´ç»“æœï¼Œåå‘ä¼ æ’­ä¹‹åï¼Œè¿™äº›ç¼“å­˜å°±è¢«æ¸…ç©ºï¼Œå¯é€šè¿‡æŒ‡å®šè¿™ä¸ªå‚æ•°ä¸æ¸…ç©ºç¼“å­˜ï¼Œç”¨æ¥å¤šæ¬¡åå‘ä¼ æ’­ã€‚ï¼ˆä¸‹é¢ä¼šå†ä»‹ç»ä¸€éï¼‰
> - **create_graph**ï¼šå¯¹åå‘ä¼ æ’­è¿‡ç¨‹å†æ¬¡æ„å»ºè®¡ç®—å›¾ï¼Œå¯é€šè¿‡`backward of backward`å®ç°æ±‚é«˜é˜¶å¯¼æ•°ã€‚
>

#### è‡ªåŠ¨æ±‚å¯¼ç¤ºä¾‹

é€šè¿‡ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥çœ‹çœ‹`PyTorch`ä¸­çš„è‡ªåŠ¨æ±‚å¯¼æœºåˆ¶ã€‚

```python
import torch
from torch.autograd import Variable

### EXAMPLE 1
# x,wç­‰éƒ½æ˜¯æ ‡é‡
x = Variable(torch.FloatTensor([1]), requires_grad=True)
w = Variable(torch.FloatTensor([2]), requires_grad=True)
b = Variable(torch.FloatTensor([3]), requires_grad=True)


# Build a computational graph.
y = w * x + b    # y = 2 * x + 3

# Compute gradients.
y.backward()

# Print out the gradients.
print(x.grad)    # x.grad = 2
print(w.grad)    # w.grad = 1
print(b.grad)    # b.grad = 1
```

> ä¸ºä»€ä¹ˆæˆ‘ä¸€å¼€å§‹æ²¡æœ‰ç”¨éæ ‡é‡ï¼Œå› ä¸ºéæ ‡é‡åå‘æ±‚å¯¼ï¼ˆbackwardï¼‰çš„æ—¶å€™è¦è®¾å®šè¾“å‡ºtensorçš„å½¢çŠ¶ã€‚

```python
import torch
from torch.autograd import Variable

### EXAMPLE 2
a = Variable(torch.ones(3, 4), requires_grad=True)
b = Variable(torch.zeros(3, 4))
c = a.add(b)
d = c.sum()
# ğŸ‘†æ‰€æœ‰é¡¹åŠ èµ·æ¥æ˜¯12ï¼Œå› ä¸ºæ˜¯variableç›¸åŠ ï¼Œæ‰€ä»¥dä¹Ÿæ˜¯variableã€‚
# è¿™ä¹Ÿè¯´æ˜äº†c.sum()æ˜¯variable
# è€Œc.data.sum()æ˜¯tensor

d.backward()	# åå‘ä¼ æ’­

print(a.grad)	# dï¼ˆå³sum(a+b)ï¼‰å¯¹aæ±‚å¯¼ï¼Œè¿”å›ones(3, 4)
print(a.requires_grad, b.requires_grad, c.requires_grad)
# (True, False, True)
# å› ä¸ºæˆ‘ä»¬ä½¿ç”¨äº†a.gradï¼Œæ‰€ä»¥a.requires_gradä»Falseå˜ä½Trueã€‚é‚£ä¹ˆä¸ºä»€ä¹ˆc.requires_gradä¹Ÿæ˜¯Trueå‘¢ï¼Œèªæ˜çš„äººçŸ¥é“ï¼Œè¿™æ˜¯ç”±äºä½¿ç”¨äº†é“¾å¼æ³•åˆ™ï¼Œå¯¹aæ±‚å¯¼çš„æ—¶å€™ï¼Œå…ˆä¼šè®©då¯¹c(å³a+b)æ±‚å¯¼ã€‚

print(a.is_leaf, b.is_leaf, c.is_leaf)	# åˆ¤æ–­æ˜¯å¦ä¸ºå¶å­èŠ‚ç‚¹
# (True, True, False)

# c.gradæ˜¯None, å› cä¸æ˜¯å¶å­èŠ‚ç‚¹ï¼Œå®ƒçš„æ¢¯åº¦æ˜¯ç”¨æ¥è®¡ç®—açš„æ¢¯åº¦
# æ‰€ä»¥è™½ç„¶c.requires_grad = True,ä½†å…¶æ¢¯åº¦è®¡ç®—å®Œä¹‹åå³è¢«é‡Šæ”¾
print(c.grad is None)	# True

# grad_fnå¯ä»¥æŸ¥çœ‹è¿™ä¸ªvariableçš„åå‘ä¼ æ’­å‡½æ•°ï¼Œ
# cæ˜¯addå‡½æ•°çš„è¾“å‡ºï¼Œæ‰€ä»¥å®ƒçš„åå‘ä¼ æ’­å‡½æ•°æ˜¯AddBackward
print(c.grad_fn)
# <ThAddBackward object at 0x109217438>
```

åœ¨`grad_fn`ä¸­ï¼Œ`next_functions`ï¼ˆä¸€ä¸ªå…ƒç»„`tuple`ï¼‰ä¿å­˜`grad_fn`çš„è¾“å…¥ã€‚ï¼ˆå¶å­èŠ‚ç‚¹éƒ½æ²¡æœ‰`grad_fn`ï¼‰

```python
import torch
from torch.autograd import Variable

x = Variable(torch.ones(1))
b = Variable(torch.rand(1), requires_grad=True)
w = Variable(torch.rand(1), requires_grad=True)
y = w*x
z = y+b

print(z.grad_fn)	# <ThAddBackward object at 0x109217438>
print(z.grad_fn.next_functions)
# ((<ThMulBackward object at 0x1080ea358>, 0), (<AccumulateGrad object at 0x1080ea4a8>, 0))
# å¦‚ä½•è§£é‡Šè¿™ä¸ªnext_functionså‘¢ï¼Ÿå…¶å®å°±æ˜¯å¯¹åº”z=y+bç›¸å…³çš„ä¸¤ä¸ªå˜é‡yå’Œbã€‚yæ˜¯é€šè¿‡â€œä¹˜â€å¾—åˆ°çš„ï¼Œæ‰€ä»¥æ˜¯`ThMulBackward`ï¼Œè€Œbæ˜¯ä¸€ä¸ªå¶å­èŠ‚ç‚¹ï¼Œæ˜¯ç›´æ¥åˆå§‹åŒ–çš„ï¼Œæ‰€ä»¥æ˜¯`AccumulateGrad`ï¼ˆè¯¥æ ‡è¯†ä¹Ÿæœ‰å¯èƒ½æ˜¯æ¢¯åº¦ç´¯åŠ çš„æ„æ€ï¼‰ã€‚è¿™é‡Œçš„`next`æ˜¯æŒ‰ç€ä¼ æ’­çš„åæ–¹å‘
```



## è®¡ç®—å›¾

ä»€ä¹ˆæ˜¯è®¡ç®—å›¾ï¼Ÿ

![img](https://pic3.zhimg.com/v2-5d56dc3930e0c68b2f1655b18265f172_b.gif)

â€‹	åœ¨ç ”ç©¶ç¥ç»ç½‘ç»œçš„æ—¶å€™ï¼Œæˆ‘ä»¬å¸¸é€šè¿‡è®¡ç®—å›¾æ¥ä½¿æ•´ä¸ªç¥ç»ç½‘ç»œå˜å¾—æ¡ç†æ¸…æ™°ä¸”ç›´è§‚ã€‚ä¸€ä¸ªè®¡ç®—å›¾ç”±**èŠ‚ç‚¹**ï¼ˆNodeï¼Œé€šå¸¸æ¥è¯´é‡Œé¢æ”¾çš„æ˜¯å˜é‡ï¼‰å’Œ**è¿æ¥çº¿**ï¼ˆEdgeï¼Œä¸€èˆ¬æ˜¯è¿ç®—æ“ä½œï¼‰ç»„æˆã€‚è®¡ç®—å›¾åœ¨BPNNä¸­æä¸ºé‡è¦ã€‚æˆ‘ä»¬ä»é«˜æ•°ä¸­çš„æ±‚å¤šå…ƒå‡½æ•°åå¯¼æ•°å¼€å§‹è®²è§£è®¡ç®—å›¾æ˜¯ä»€ä¹ˆä¸œä¸œï¼š
$$
\begin{aligned}
&e=(a+b)\times(b+1)\\
&ä»¤c=a+b\\
&d=b+1\\
&æ±‚\frac{\partial e}{\partial a}å’Œ\frac{\partial e}{\partial b}:\\
\frac{\partial e}{\partial a}&=\frac{\partial e}{\partial c}\frac{\partial c}{\partial a}+\frac{\partial e}{\partial d}\frac{\partial d}{\partial a}=b+1\\
&\frac{\partial e}{\partial b}åŒç†
\end{aligned}
$$
<u>å…¶å¯¹åº”çš„è®¡ç®—å›¾å¦‚ä¸‹ï¼š</u>

![p1](https://ws4.sinaimg.cn/large/006tKfTcly1ftwjwuupqbj319i0us4qp.jpg)

`Autograd`çš„ç‰›é€¼ä¹‹å¤„ï¼Œä¹Ÿåœ¨äºå…¶èƒ½è‡ªåŠ¨ç”Ÿæˆ`Computational Graph`ã€‚



### Buffer

åœ¨BPNNä¸­è®¡ç®—æ¢¯åº¦çš„æ—¶å€™ï¼Œè¿™äº›æ•°å€¼ä¼šåœ¨å‘å‰ä¼ æ’­çš„è¿‡ç¨‹ä¸­è¢«å­˜åœ¨bufferï¼ˆç¼“å†²åŒºä¸­ï¼‰ä¸­ï¼Œæ¯ä¸€æ¬¡åå‘ä¼ æ’­å®Œæˆåéƒ½ä¼šå°†ä¹‹æ¸…ç©ºã€‚åœ¨å¤šæ¬¡åå‘ä¼ æ’­çš„è¿‡ç¨‹ä¸­ï¼Œå¦‚æœè¦ä¿å­˜è¿™äº›å€¼ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å®š`retain_graph`æ¥ä¿ç•™è¿™äº›bufferã€‚

```python
import torch
from torch.autograd import Variable

x = Variable(torch.rand(1), requires_grad=True)
# è®°å¾—è®¾ç½®requires_gradå¦åˆ™æ— æ³•æ±‚å¯¼ï¼Œè€Œä¸”
y = 2*x
z = x+y

z.backward(retain_graph=True)
x.grad
```



## PyTorch 0.4

å…¶å®åœ¨0.4ç‰ˆæœ¬å½“ä¸­ï¼Œ`Tensor`å’Œ`Variable`å·²ç»èåˆäº†ã€‚

> `torch.Tensor`å’Œ`torch.autograd.Variable`ç°åœ¨å…¶å®æ˜¯åŒä¸€ä¸ªç±»! æ²¡æœ‰æœ¬è´¨çš„åŒºåˆ«! æ‰€ä»¥ä¹Ÿå°±æ˜¯è¯´,Â **ç°åœ¨å·²ç»æ²¡æœ‰çº¯ç²¹çš„Tensoräº†, æ˜¯ä¸ªTensor, å®ƒå°±æ”¯æŒè‡ªåŠ¨æ±‚å¯¼ï¼**Â ç°åœ¨å†ä¹Ÿä¸è¦ç»™`Tensor`åŒ…ä¸€ä¸‹`Variable`ï¼Œå› ä¸ºå®ƒå·²ç»æ˜¯å¤©ç”Ÿçš„`Variable`äº†ã€‚

> å…³äº`PyTorch`çš„ä»£ç è¿ç§»ï¼Œè¯·ç­‰æœ‰ä¸€å®šåŸºç¡€åå‚ç…§[è¿™é‡Œ](https://www.itency.com/topic/show.do?id=494122)ã€‚å› ä¸ºå¾ˆå¤šæ•™ç¨‹æ˜¯åŸºäº0.3çš„ï¼Œä¹‹æ‰€ä»¥è¦ç©¿æ’0.3çš„æ–¹æ³•ï¼Œæ˜¯å¸Œæœ›è¯»è€…åŒæ ·èƒ½è¯»æ‡‚0.3çš„ä»£ç ã€‚



> **REF.**
>
> [chenyuntc](https://github.com/chenyuntc/pytorch-book/blob/master/chapter3-Tensor%E5%92%8Cautograd/Autograd.ipynb)
>
> [yunjey](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py)
>
> [csdn](https://blog.csdn.net/u013527419/article/details/70184690)
>
> [optimizer](https://ptorch.com/news/54.html)