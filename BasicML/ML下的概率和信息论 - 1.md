# ML下的概率和信息论 - 1



## 为什么

​        **首先学习一个东西，我们得知道为什么要用它。**（别和我说是考研要考）

​	概率论是用于表示不确定性 *声明*(statement)的数学框架。它不仅提供了量化 不确定性的方法，也提供了用于导出新的不确定性声明的公理。在人工智能领域，概率论主要有两种用途。首先，概率法则告诉我们 AI 系统如何推理，据此我们设计一些算法来计算或者估算由概率论导出的表达式。其次，我们可以用概率和统计从理论上分析我们提出的 AI 系统的行为。

​	计算机科学许多分支都有“确定且必然”的，比如在c++中`int a=1`，那么a便是1了，这是确定且必然的。硬件错误确实会发生，但它们足够罕见，以致于大部分软件应用在设计时并不需要考虑这些因素的影响。鉴于许多计算机科学家和软件工程师在一个相对干净和确定的环境中工作，机器学习对于概率论的大量使用是很令人吃惊的。

​	这是因为机器学习通常必须处理不确定量，有时也可能需要处理随机 (非确定性的) 量。不确定性和随机性可能来自多个方面。 几乎所有的活动都需要能够在不确定性存在时进行推理。事实上，除了那些被定义为真的数学声明，我们很难认定某个命题是千真万确的或者确保某件事一定会 发生。 

> **不确定性**的来源：
>
> * <u>被建模系统内在的随机性。</u>例如，大多数量子力学的解释，都将亚原子粒子的动力学描述为概率的。我们还可以创建一些我们假设具有随机动态的理论情境， 例如一个假想的纸牌游戏，在这个游戏中我们假设纸牌真正混洗成了随机顺序。 
> * <u>不完全观测。</u>即使是确定的系统，当我们不能观测到所有驱动系统行为的变量时，该系统也会呈现随机性。例如，在 Monty Hall 问题中，一个游戏节目的参赛者被要求在三个门之间选择并且赢得放置在选中门后的奖金。两扇门通向山羊，第三扇门通向一辆汽车。选手选择所导致的结果是确定的，但是站在选手 的角度，结果是不确定的。
> * <u>不完全建模。</u>当我们使用一些必须舍弃某些观测信息的模型时，舍弃的信息会导致模型的预测出现不确定性。例如，假设我们制作了一个机器人，它可以准确地观察周围每一个对象的位置。如果预测这些对象将来的位置时机器人采用的是离散化的空间，那么离散化使得机器人立即变得不能确定对象的精确位置：每个对象都可能处于它被观察到占据的离散单元的任何位置。（而且有的时候，一方面，我们为了便利，牺牲部分结果质量而简化模型，另一方面，就像Marian Bantjes说的"*All* *models* *are* *wrong*, but some *are* useful."，没有百分之百试用的万能模型，只是其中的部分模型“有用“而已）

​	当我们说一个结果发生的概率为 p，就意味着如果我们反复实验 (例如，抽取一手牌) 无限次，有 p 的比例会导致这样的结果。这种推理似乎并不立即适用于那些不可重复的命题。如果一个医生诊断了病人，并说该病人患流感的几率为 40%，这意味着非常不同的事情——我们既不能让病人有无穷多的副本，也没有任何理由去相信病人的不同副本在具有不同的潜在条件下表现出相同的症状。在医生诊断病人的情况下， 我们用概率来表示一种 **信任度(degree of belief)**，其中 1 表示非常肯定病人患有流感，而 0 表示非常肯定病人没有流感。前面一种概率，直接与事件发生的频率相联系，被称为 **频率派概率(frequentist probability)**;而后者，涉及到确定性水平，被称为 **贝叶斯概率(Bayesian probability)**。

​	概率可以被看作是用于处理不确定性的逻辑扩展。逻辑提供了一套形式化的规则，可以在给定某些命题是真或假的假设下，判断另外一些命题是真的还是假的。概率论提供了一套形式化的规则，可以在给定一些命题的似然后，计算其他命题为真的似然。

> 我现在做一个假设，我有95%的信任度觉得，看这篇文章的同学都上过高中（所以有一定的概率论基础）。但是大学教的概率论必定更高一层，我们就来学习一下这更高一层的东西。



## 概率分布

### 离散型变量和概率质量函数

​         <u>离散型变量的概率分布</u>可以用 **概率质量函数(probability mass function, PMF)**来描述。 **P**表示概率质量函数。通常每一个随机变量都会有一个不同的概率质量函数，并且读者必须根据随机变量来推断所使用的 **PMF**，而不 是根据函数的名称来推断；例如，P(x) 通常和 P(y) 不一样。（这里不要把我们说的变量看成单个的数值，其也有可能是一系列数值，也有可能不是数值...）

​         **概率质量函数**将随机变量能够取得的每个状态映射到随机变量取得该状态的概率。比如， $P(x_i)$代表x在$x_i$状态下的概率，概率为 1 表示x = $x_i$ 是确定的，概率为 0 表示 x = $x_i$ 是不可能发生的。 有时为了使得PMF的使用不相互混淆，我们会明确写出随 机变量的名称:$P$ (x = $x$)。有时我们会先定义一个随机变量，然后用 $\sim$ 符号来说明它遵循的分布:x $\sim P (x)$。

​         **概率质量函数**可以同时作用于多个随机变量。这种多个变量的概率分布被称 为 **联合概率分布(joint probability distribution)**。$P$ (x = $x$, y = $y$) 表示 x = $x$ 和 y =$y$ 同时发生的概率。我们也可以简写为 $P (x, y)$。

​	如果一个函数$P$是随机变量 x 的$PMF$，必须满足下面这几个条件：

> * P的定义域必须是 x 所以可能状态的集合。
> * $∀x ∈$ x$, 0 ≤ P (x) ≤ 1 $. 
> * **归一化**：对一个随机变量而言，所有状态对应的概率之和为1。

​	假设一个离散型随机变量 x 有 k 个状态，且其服从**均匀分布(uniform distribution)**（所有状态为等可能的），那么其PMF：
$$
P({\rm x} = x_i)=\frac{1}{k}
$$


### 连续性变量和概率密度函数

​       PMF是对应离散型变量的，而**概率密度函数(PDF)**是对应连续性随机变量的。如果函数$p$是概率密度函数，那么：

> * $p$ 的定义域是 x 所以可能状态的集合
> * 其在定义域积分为0
> * $∀x ∈$ x$,p(x) ≥ 0$. 注意，我们并不要求 $p(x) ≤ 1 $

$p(x)$并没有直接给出对应状态的概率，相对的，给出了落在$lim_{\sigma\sim0}\ (x±\sigma)$那块极小区域（x附近一小块区域）的概率。一般我们研究$x$落在$[x_i,x_j]$区域的概率，即$p(x)$在对应区域对x的积分。

​	举一个实数区间**均匀分布**的例子：$u(x;a,b)$表示以$x$为自变量，$[a,b]$为定义域的概率（定义域之外，概率为0），定义域上任意一点的概率密度为：
$$
u(x;a,b)=\frac{1}{b-a}
$$
其在定义域上的定积分为1。



## 边缘概率

​	这是针对一组变量的**联合概率分布**而言的，对于$P({\rm x}=x,{\rm y}=y)$，若我们想知道其子集，比如说随机变量x的概率（也就是我们说的边缘概率）：
$$
\forall x \in {\rm x}, P({\rm x}=x) = \sum_yP({\rm x}=x,{\rm y}=y)
$$
对于连续型，即对多元函数的某（多）个变量求积分即可。



## 条件概率

​	比如在A事件发生的情况下，我们计算B事件发生的概率，即$P(B|A)=\frac{P(AB)}{P(A)}$。

​	这里需要注意的是，不要把条件概率和计算当采用某个动作后会发生什么相混 淆。假定某个人说德语，那么他是德国人的条件概率是非常高的，但是如果随机选择的一个人会说德语，他的国籍不会因此而改变。计算一个行动的后果被称为 **干预查询(intervention query)**。干预查询属于 **因果模型(causal modeling)** 的范畴。



## 独立性和条件独立性

​	两个随机变量 x 和 y，如果它们的概率分布可以表示成两个因子的乘积形式，并 且一个因子只包含 x 另一个因子只包含 y，那我们就称这两个随机变量是 **相互独立的(independent)**。表示为$x\perp y$。



​	若x、y在满足${\rm z}=z$的情况下才”相互独立“，那么他们是在给定随机变量$z$时是 **条件独立** 的:
$$
\forall x \in x,y\in y,z\in z, p({\rm x}=x,{\rm y}=y|{\rm z}=z)=p({\rm x}=x|{\rm z}=z)p({\rm y}=y|{\rm z}=z)
$$
表示为$x\perp y\ |\ z$。


## 贝叶斯规则

在已知$P(x|y)$和$P(x)$（同样也get了$P(y)$）的情况下计算$P(y|x)$：
$$
P(x|y)= P(x)\frac{P(y|x)}{P(y)}
$$


 感性的解释可以点击马同学的[回答](https://www.zhihu.com/question/19725590)。

贝叶斯规则是这么一回事：

`新信息出现后x的概率 = 原来x的概率 × 新信息带来的调整`

> **推导：**
> $$
> P(x|y)P(y)=P(x\cap y)=P(y|x)P(x)
> $$
>

> $P(x)$叫做**先验**，$P(x|y)$叫做**后验**。$P(y|x)$叫做**似然**，表示在<u>承认先验的条件下</u>另外一个与之相关的随机变量的表现。

> 那么我们来做个题：
>
> > 男生有10%的概率是长发，女生有70%的概率是长发，一个公园里有60%的男生，40%的女生，请问如果您在公园里遇到一个长发的人，其为男生的概率是多少？
> >
> > **解：**
> >
> > 先验概率$P(sex_1)=0.6,P(sex_2)=0.4$
> >
> > 似然$P(longHair|sex_1)=0.1,P(longHair|sex_w)=0.7$
> >
> > 男女长发是相互独立的
> > $$
> > P(longHair)=P(longHair|sex_1)P(sex_1)+P(longHair|sex_2)P(sex_2)=0.34
> > $$
> > 由贝叶斯公式：
> > $$
> > P(sex_1|longHair)=P(sex_1)\frac{P(longHair|sex_1)}{P(longHair)}=\frac{0.6\times 0.1}{0.34}=\cdots
> > $$
> >

## 期望、方差和协方差

> 期望和方差高中也有涉及，不过过于简单。

#### 期望:

* **离散型**：$\mathbb{E}_{{\rm x\sim P}}[f(x)]=\sum_xP(x)f(x)$

  > 可以把x看作序号，$f(x)$看作序号对应的值，期望就是$f(x)$在该分布下的平均值

* **连续型**：$\mathbb{E}_{{\rm x\sim P}}[f(x)]=\int p(x)f(x)dx$

#### 方差:

$$
Var(f(x))=\mathbb{E}[(f(x)-\mathbb{E}f(x))^2]\\
为了化简，把f(x)写成x,则：\\
Var(x)=\mathbb E[x^2]-(\mathbb E[x])^2\\
Var(ax)= a^2Var(x)
$$

> 方差的平方根是标准差

#### 协方差:

> 反应了两个边路线性相关小的强度以及这些边路的尺度

$$
Cov(f(x),g(y))=\mathbb{E}[(f(x)-\mathbb E[f(x)])(g(y)-\mathbb E[g(y)])]
$$

这样写可能更加方便理解：$Cov(X,Y)=\frac{1}{m}\sum^m_{i=1}(x_i-\overline x)(y_i-\overline y)$

> <u>协方差的绝对值如果很大</u>则意味着变量值变化很大并且它们同时距离各自的均值很远。
>
> 如果<u>协方差是正</u>的，那么两个变量都倾向于同时取得相对较大的值。
>
> 如果<u>协方差是负</u>的，那么其中一个变量倾向于取得相对较大的值的同时，另一个变量倾向于 取得相对较小的值，反之亦然。 

**相关系数**是用以反映<u>变量之间相关关系密切程度</u>的统计指标。

> 相关系数也可以看成协方差：一种剔除了两个变量量纲影响、<u>标准化</u>后的特殊协方差,它消除了两个变量变化幅度的影响，而只是<u>单纯反应两个变量每单位变化时的相似程度</u>。
> $$
> r=\frac{Cov(X,Y)}{\sqrt{\sigma_x \cdot \sigma_y}}
> $$
>

> **ATTENTION:**
>
> 协方差和相关性是有联系的，但实际上不同的概念。它们是有联系的，因为<u>两个变量如果相互独立那么它们的协方差为零</u>，如果两个变量的协方差不为零那么它们一定是相关的。然而，独立性又是和协方差完全不同的性质。两个变量如果协方差为零，它们之间一定没有线性关系。独立性是比零协方差的要求更强，因为独立性还排除了非线性的关系。两个变量相互依赖但是具有零协方差是可能的。
>
> ​         **随机向量** $x\in \mathbb{R}^n$的**协方差矩阵**是一个$n\times n$的矩阵，并且满足：
> $$
> Cov({\rm x})_{i,j}=Cov(x_i,x_j)
> $$
> 其中$x_i,x_j$都是向量。
>
> 而且自己和自己的协方差(协方差矩阵的对角元)就是其本身的方差：
> $$
> Cov(x_i,x_i)=Var(x_i)
> $$
>
