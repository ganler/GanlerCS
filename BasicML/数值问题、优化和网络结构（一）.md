# 数值问题、优化和网络结构（一）

> Mainly about:
>
> - 激活函数的细节问题（特别是ReLU及其衍生品）
> - Xavier初始化方法
> - BN&dropout方法
> - 凸优化方法

## 数值问题

> 	在深度学习当中，数值问题是一个非常重要的问题。因为网络十分deep，就算是很小的数值，在网络传播当中都有可能发生翻天覆地的变化。以及，我们所讨论的deep learning是要能在计算机中计算的，而计算机的数值是有范围的，精度也是有限的......所以我们现在要着重讨论的问题是：如何使用一些方法来解决数值问题（数值溢出，梯度消失等等）。



## 激活函数

	虽然这是个老生常谈的问题，但如果不强调的话，可能读到最后会产生一个“既然激活函数这么麻烦，那还不如不要激活函数”的疑问。我们要知道，激活函数一定要是非线性的，不然就没意义了，而为什么要是非线性的呢，因为神经网络的非线性能力就是由激活函数提供的呀~即激活函数的作用：提供非线性能力。（想看看具体老外的解释可以参考下文，摘自[Learn OpenCV](https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/)）

> **Why do we need a non-linear activation function in an artificial neural network?**
>
> Neural networks are used to implement complex functions, and non-linear activation functions enable them to approximate arbitrarily complex functions. Without the non-linearity introduced by the activation function, multiple layers of a neural network are equivalent to a single layer neural network.
>
> Let’s see a simple example to understand why without non-linearity it is impossible to approximate even simple functions like XOR and XNOR gate. In the figure below, we graphically show an XOR gate. There are two classes in our dataset represented by a cross and a circle. When the two features, ![X1](https://www.learnopencv.com/wp-content/ql-cache/quicklatex.com-95a6237cd078bb2c77dfc30c0ec674e9_l3.png) and ![X2](https://www.learnopencv.com/wp-content/ql-cache/quicklatex.com-71a291e804fecf93499e1ad302abc279_l3.png) are the same, the class label is a red cross, otherwise, it is a blue circle. The two red crosses have an output of 0 for input value (0,0) and (1,1) and the two blue rings have an output of 1 for input value (0,1) and (1,0).
>
> [![img](https://www.learnopencv.com/wp-content/uploads/2017/10/xor.png)](https://www.learnopencv.com/wp-content/uploads/2017/10/xor.png)
>
> Figure: Graphical Representation of XOR gate
>
> From the above picture, we can see that the data points are not linearly separable. In other words, we can not draw a straight line to separate the blue circles and the red crosses from each other. Hence, we will need a non-linear decision boundary to separate them.
>
> The activation function is also crucial for squashing the output of the neural network to be within certain bounds. The output of a neuron ![\sum^n_i w_i x_i + b](https://www.learnopencv.com/wp-content/ql-cache/quicklatex.com-206fd95e028a0979d718a50209c266d0_l3.png) can take on very large values. This output, when fed to the next layer neuron without modification, can be transformed to even larger numbers thus making the process computationally intractable. One of the tasks of the activation function is to map the output of a neuron to something that is bounded ( e.g., between 0 and 1).
>
> With this background, we are ready to understand different types of activation functions.

#### Sigmoid

$$
\sigma(z)=\frac{1}{1+\exp(-z)}\in(0,1)\\
\sigma'=\sigma(1-\sigma)\in(0,\frac{1}{4}]
$$

图像大家心里都有数，其将任意大小的实数映射到$(0,1)$，可用于二分类。

> **缺点**：
>
> - 梯度消失（梯度弥散）：$\sigma'\le\frac{1}{4}$，这在浅层网络没什么，在深层网络很致命。$\sigma^{+∞}=0$，在反向传播中多次传播，就算一个大的数值都被传成了一个极小的数值。这样会使整个网络的学习十分低效。
> - 计算不简单：有除法，还有幂运算。

#### Tanh

$$
\tanh(z)=\frac{\exp(z)-\exp(-z)}{\exp(z)+\exp(-z)}=2\sigma(2z)-1\in(-1,1)
$$

![img](https://s1.ax1x.com/2018/08/23/P7wWBd.png)
$$
\tanh'(z)=4\sigma'(2z)=4\sigma(2z)(1-\sigma(2z))\in(0,1]
$$


![img](https://s1.ax1x.com/2018/08/23/P7wbjg.png)

> 	尽管如此，$\tanh(x)$的导数还是小于等于0，多次误差反向传播后得到的值还是可能趋近于0。

#### ReLU(Rectified Linear Unit 整流线性单元)

	这个是deep learning中最火热的函数，因为其在大于0的区域，导数为1，所以网络多深，传播的效率不会被降低。而且计算会很简单。
$$
ReLU(x)=\max(0,x)
$$

> 导数我就不计算了。。。

> **缺点**
>
> - **Dead Neurons**: 想象一个神经元，当其输出的x<0的时候，经过ReLU得到的结果为0，导数也为0，也意味着在误差反向传播经过该神经元的时候，不管误差多大，都会乘上这个0，导致这个反向的误差被"kill"，对应的参数没有学习（即没有更新参数），也就是为什么我们称之为“死亡神经元”。

### Leaky ReLU

$$
leaky\ Relu(x)=\max(\alpha x,x)\\
\alpha<1,usually,\ we\ set\ it\ 0.01.
$$

> 这样，就算在x<0的情况下，也不会导致"dead neurons"。



## 初始化

> 初始化的不当会导致一些很严重的问题。假想我们使用ReLU，而权重参数的初始化导致被ReLU激活的输出全为0，那么不管模型如何训练都会非常难看。或者说如果一开始的参数方差>1，在深层网络中，这样会导致随着网络传播，output的方差也越来越大，然后就导致数值溢出了（方差<1则渐渐导致梯度弥散）。针对第二个问题，如果能让这个方差一直不变，那就问题解决了。

![img](https://s1.ax1x.com/2018/08/23/P76pCR.png)

### Xavier initialization

Xavier的目的就是保持每层输出的<u>方差不变</u>，来抵御数值溢出和梯度弥散。

假设参数矩阵的size为(m×n)，那么我们:
$$
\mathbb W=[-\sqrt\frac{6}{m+n}, \sqrt\frac{6}{m+n}]\\
We\ use\ [\rm uniform(\mathbb W)]\ for\ intialization.
$$
即使用$\mathbb W$的均匀分布来初始化参数。

Xavier initialization基于以下前提：

> - 忽略偏置影响；
> - 激活函数为$\tanh (x)$，而该函数在x趋向0的地方和$y=x$等价无穷小，所以我们将$\tanh(x)$当做$f(x)=x$；
> - 参数之间相互独立。

> **推导**：
> $$
> \begin{aligned}
> &Assuming\ that:\\
> &我们有\\
> &第n个随机变量向量x_i(output),其均值为\mu_x=0,标准差为\sigma_x\\
> &第n个权重矩阵w_n,\mu_{w_n}=0,\sigma_{w_n}
> \\&按照假设，w和x之间相互独立(举个例子\sigma^2_{2wx}=2\sigma^2_{wx}) \\ \\
> &在卷积层的情况下：\\
> &x_2=sum(w_1x_1)\\
> &\mu_{x_2}=0,\sigma_{x_2}=l\times\sigma_{x_1}\sigma_{w_1}\\
> &(l为x_i的长度,或者说w_i这个filter的边长)\\
> \\&多层网络下来\\
> &\sigma_{x_n}=\sigma_{sum(w_{n-1}x_{n-1})}=l_{n-1}\sigma_{(w_{n-1})}\sigma_{(x_{n-1})}=\\&\cdots\\
> &=\sigma_{x_1}\prod_{i=1}^{n-1}l_i\sigma_{w_i}
> \end{aligned}
> $$
> 重申一下我们的目的：**保持每层输出的方差<u>稳定不变</u>**。那么即$\sigma_{x_i}=\sigma_{x_j}$。即$\sigma_{w_i}=\frac{1}{ l_i}$。
>
> 是前向传播，那么后向呢？
>
> ![img](https://s1.ax1x.com/2018/08/23/P76oZD.png)
> $$
> \nabla x_j^{[k-1]}=\frac{\partial Loss}{\partial x^{[k-1]}_j}=\sum_{i=1}^l\frac{\partial Loss}{\partial x_i}w_{j\to i}
> $$
> 按照上式，根据前向传播的过程，同样，我们得到:
> $$
> \sigma(\nabla x_j^{[1]})=\sigma(\nabla x_i^{[n]})\prod_{i=1}^{n-1}\sigma_{w_i}\hat{l_i}
> $$
> 这里的$\hat{l}$是将网络反过来后的layer感知器数。
>
> 同样我们也要保证每层的$\sigma(\nabla x)$不变。那么$\sigma_{w_i}=\frac{1}{\hat l_i}$。
>
> 这里$l和\hat l$分别可以看在前向传导过程中做前一排神经元传到后一排神经元的过程中，前一排神经元数量和后一排神经元数量。我们整合$\sigma_{w_i}=\frac{1}{ l_i}$$和\sigma_{w_i}=\frac{1}{\hat l_i}$，得到$\sigma_{w_i}=\frac{2}{\hat l_i+l_i}$。
>	
> 对于一个在[a, b]区间的均匀分布变量，其方差为：$\frac{(b-a)^2}{12}$。
>		
> 对于一个在[-a, a]区间的均匀分布变量，令之方差为$\frac{1}{m+n}$，可得$a=\sqrt\frac{6 }{m+n}$。
>		
> 这就是Xavier初始化的推导。



## Batch Normalization and Dropout

说了这么多，还是BN最好用~

> 这些方法在我的另外一篇[文章](https://ganler.github.io/2018/08/21/PyTorch-Learning-Note-FNN-and-CNN/)中有提到，不赘述。



## Optimizer

> 我的这篇[文章](https://ganler.github.io/2018/08/21/PyTorch-Learning-Note-Before-nerual-network/)有提及