# 数值问题、优化和网络结构（二）

> 主要讨论降维问题

## 降维

> 何为维度？${\rm x}=[x_1,x_2,x_3,x_4]$就比${\rm y}=[y_1,y_2,y_3]$的维度要高。（4>3）
>
> 维度很高会导致一些问题：
>
> - 计算代价更大，消耗更多的计算资源（拿knn作为例子，没增加一个维度都意味着巨大的计算量）；
> - 防止过拟合（所谓的“信息丰富，只是贫乏”，信息量太多但没有区分度，反而会导致过拟合）;
> - 难以可视化，去除噪声（高维度的数据难以被可视化）；
>
> 降维就意味着要对维数进行压缩。

将$\{x_i\}^n_{i=1}$这个输入样本变换为低维数据$\{z_i\}^n_{i=1}$，即从原来特征中选出最为主要的特征。

如此图，我们可以对下列密集的坐标点画坐标轴，但是如何画最好呢，其中我们选择B，因为这个方向上，数据集的差异最大，数据的方差最大，也更有区分度，这就为我们分类提供了便利。

![img](https://s1.ax1x.com/2018/08/25/PHreYR.png)

![img](https://s1.ax1x.com/2018/08/25/PHrBnS.png)



> 摘自《实战机器学习》

降维方法中最简单的就是**线性降维**：
$$
z_i=Tx_i
$$
假设x的维度为m的列向量，z的维数为n的列向量，那么T就是一个n×m的矩阵。这样的线性降维其实是原来n维空间向量对m维空间向量投影所导致。

### 主成分分析法(PCA)

> **What we're going to do:**
>
> PCA为了找出主要的维度，会在一堆数据上去画若干个新的坐标轴。其中，第一个新坐标轴选择是<u>原始数据中**方差最大**的方向</u>，第二个新坐标轴选取是与<u>第一个坐标轴正交的平面中</u>使得**方差最大**的，第三个轴是<u>与第1,2个轴正交的平面中</u>**方差最大**的。依次类推，可以得到n个这样的坐标轴。通过这种方式获得的新的坐标轴，我们发现，大部分方差都包含在前面k个坐标轴中，后面的坐标轴所含的方差几乎为0。于是，我们可以忽略余下的坐标轴，只保留前面k个含有绝大部分方差的坐标轴。事实上，这相当于只保留包含绝大部分方差的维度特征，而忽略包含方差几乎为0的特征维度，实现对数据特征的降维处理。

> 这些我们可以通过分析数据集的**协方差矩阵**及其**特征分解**解决（基于特征值分解协方差矩阵实现PCA算法）。

首先我们要明白一些公式：

> $x_1,\cdots,x_i,\cdots,x_m$ 是样本，可看作一个个行向量。而样本$x_i$的feature分别为$X_1,X_2,\cdots,X_n$可看作一个个列向量。
>
> **样本均值**
> $$
> \mu_x=\frac{\sum_{i=1}^n x_i}{n}
> $$
> **中心化**
> $$
> X_i=X_i-\mu_{X_i}
> $$
> **样本方差**
> $$
> \sigma_i^2 =\frac{\rm sum[(X_i-\mu_{X_i})^2]}{n-1}
> $$
> **协方差**
>
>
> $$
> Cov(X_1,X_2)=\frac{\sum_{i=1}^m(X_1-\mu_{X_1})(X_2-\mu_{X_2})^T}{m-1}\\
> $$
> **协方差矩阵**
> $$
> X=[x_1,x_2,\cdots,x_i,\cdots,x_m]^T\\
> Cov(X,X)=\left[
> \begin{matrix}
>  Cov(x_1,x_1)      & Cov(x_1,x_2)      & \cdots & Cov(x_1,x_m)      \\
>  Cov(x_2,x_1)      & Cov(x_2,x_2)      & \cdots & Cov(x_2,x_m)      \\
>  \vdots & \vdots & \ddots & \vdots \\
>  Cov(x_m,x_1)      & Cov(x_m,x_2)      & \cdots & Cov(x_m,x_m)      \\
> \end{matrix}
> \right]
> $$
>
> - 方差的计算公式是针对一维特征，即针对同一特征不同样本的取值来进行计算得到；而协方差则必须要求至少满足二维特征；方差是协方差的特殊情况。
>
> - 方差和协方差的除数是n-1,这是为了得到方差和协方差的无偏估计。

> **将数据转换成前k个主成分的步骤：**
>
> - 中心化（所有数据减去平均值）得到$X_{(m\times n)}=\{x_1,x_2,\cdots,x_n\}$ ，其需要被降维到k维度
> - 计算协方差矩阵 $ \Sigma_{(m\times m)} = \frac{1}{n} XX^T $   
> - 计算协方差矩阵的特征值和特征向量(对称非负定矩阵必有<u>相互正交的特征向量群</u>)
>
> > $\Sigma v=\lambda v$
>
> - 将特征值从大到小排序，并对应好特征向量
>
> > $$
> > 前k个最大特征值对应的特征值：{\rm ind_k} = \arg \max_k(\lambda)\\
> > $$
> >
>
> - 保留最上面的k个特征向量(标准化后)
>
> > $$
> > 其对应的k个特征向量拼成的矩阵\\ P_{(m\times k)}=[v_1,\cdots,v_k]
> > $$
> >
>
> - 将数据转换到上述k个特征向量构建的新空间中
>
> > $$
> > Y_{(m\times k)}=XP
> > $$
> >



```python
# pca.py
from numpy import *
def loadDataSet(fileName, delim='\t'):
    fr = open(fileName)
    stringArr = [line.strip().split(delim) for line in fr.readlines()]
    datArr = [map(float ,line) for line in stringArr]
    # map(): for every line, use float for conversion.
    return mat(datArr)

def pca(X, k):
    # 中心化
    mean_X = mean(X, axis = 0)
    center_X = X-mean_X
    # 协方差矩阵
    cov_X = cov(center_X, rowvar = 0)
    # 特征分解
    eigVals, eigVects = linalg.eig(mat(cov_X))
    # 取最大k个特征值对应的特征向量凑成矩阵
    eigInd = argsort(eigVals)[::-1]
    P = eigVects[:, eigInd[:k]]
    # Y = XP
    retData = dot(center_X, P)
    return retData

input_data = array([[-1, 1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])

print(pca(input_data,1))
```

> **REF**
>
> [知乎](https://zhuanlan.zhihu.com/p/37777074)
>
> 《机器学习实战》



## ZCA白化

> **What**
>
> 对于图像而言，有很强的局部相关性，这个相关性也就意味着不同维度上（比如说图像的x轴和y轴）的变量不是独立的，如果不是独立的就很难满足一些概率论上的假设，而且相关性意味着数据是冗余的。所以我们希望通过白化过程来让输入变得：
>
> - **特征之间相关性较低**；
> - 特征之间拥有相同的方差（也有利于防止神经网络中的数值问题）。

ZCA 和 PCA 的区别：

![img](https://s1.ax1x.com/2018/08/25/PbFwxe.png)

> PCA的重点是降维；ZCA的重点是降低变量之间的相关性。
>
> ZCA放大了方差较小方向上的方差，使数据变得更加“可分”。

### How ZCA

> 让每个特征的方差变为1，特征之间的协方差变为0。

即找到一个变换W，让协方差矩阵变成同等大小的单位矩阵：
$$
Y=XW\\
即YY^T=(n-1)E
$$
**前提假设：**

> W是一个对称矩阵$W=W^T$。 

> $$
> \begin{aligned}
> &WXX^TW^T=(n-1)E\\
> &W^2XX^T=(n-1)E\\
> &W=\sqrt{(n-1)}(XX^T)^{-1/2}
> \end{aligned}
> $$
>

W的得出就是那么简单，但现在新的问题又出现了，矩阵怎么开根号？

因为$XX^T$是对称矩阵，所以其可被对角化:
$$
XX^T=S\Lambda S^{-1}
$$

> 我们知道，**对称矩阵**的特征向量相互正交：
> $$
> \lambda_1 \xi_1=A\xi_1\\
> (\lambda_1\xi_1)^T\xi_2=\xi_1^TA^T\xi_2=\xi_1^TA\xi_2=\xi_1^T\lambda_2\xi_2\\
> 又\because\ \lambda_1\neq \lambda_2\\
> \therefore \xi_1^T\xi_2=0\\
> 结论可推广
> $$
> 如果我们将**对称矩阵**的特征向量全部进行标准化（特征向量的模长都为1），那么标准化后的特征向量可以构成一个**正交矩阵**（顾名思义就是里面的列向量都是相互正交的），而且这个正交矩阵是标准正交阵。标准正交阵的一大性质就是：$S^T=S^{-1}$ 。

> 所以我们将$XX^T$“标准正交地”进行特征分解，得到的S就是一个**标准**正交阵，则有：
>
> > 因为S是“标准”的方阵，所以开方后还是本身：
>
> $$
> XX^T=S\Lambda S^T\\
> W=\sqrt{(n-1)}(S\Lambda S^T)^{-1/2}=\sqrt{(n-1)}S\Lambda^{-1/2}S^T\\
> Y=XW
> $$
>

```python
from numpy import *
import matplotlib.pyplot as plt

def zca(X):
    X -= mean(X, axis=0)
    eigVals, eigVects = linalg.eig(cov(X, rowvar = 0))
    # 在cov中已经把(n-1)给除去了
    W = eigVects.dot(linalg.inv(sqrt(diag(eigVals))).dot(eigVects.T))
    return X.dot(W)

x = random.randn(100, 1)
y = 2*x
err = 0.5*random.randn(100, 1)
y += err

data_original = hstack((x, y))
data_treated = zca(data_original)

print(f"原始数据的协方差矩阵：\n{cov(data_original-mean(data_original, axis=0), rowvar=0)}\n\nzca化后的协方差矩阵：\n{cov(data_treated-mean(data_treated, axis=0), rowvar=0)}")

plt.plot(data_original[:, 0], data_original[:, 1], 'b.', data_treated[:, 0], data_treated[:, 1], 'r*')
plt.legend(['original data', 'after zca'])

plt.show()
```

![img](https://s1.ax1x.com/2018/08/29/POXAqx.png)

```python
# 输出结果

原始数据的协方差矩阵：
[[0.91150334 1.7592528 ]
 [1.7592528  3.64988723]]

zca化后的协方差矩阵：
[[ 1.00000000e+00 -5.88610489e-16]
 [-5.88610489e-16  1.00000000e+00]]
```

是不是数据分得更开了呢~
