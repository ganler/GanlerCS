# 机器学习基础

## 凸函数及其性质

> 在机器学习中，只有下凸函数函数才叫凸函数，上凸的函数叫做凹函数。

**（下）凸函数**

C是凸集，什么是凸集？**凸集**内任意其中两点的连线还在集合内。（想象一个椭圆）
$$
for \ \ x_1,x_2\in C\\
f(\frac{x_1+x_2}{2})\le\frac{f(x_1)+f(x_2)}{2}
$$
将之推广：
$$
for \ \ \theta\in[0,1]\\
f(\theta x_1+(1-\theta)x_2)\le\theta f(x_1)+(1-\theta)f(x_2)
$$

> 如何想象？想象一个$y=x^2$曲线，上面任取两点，两点一线，线上的**任何**一个点对于的y值，都比同x坐标对应曲线的y值大。$\theta$就是用来决定这个“**任何**”是哪个点。

凸函数最大的优点，**局部最优解就是全局最优解**。$y=x^2$在$[-1,1]$的极小值就是该函数的最小值，这能理解吧~



## 目标函数

其实就是我们常说的**损失函数**。其大小表示结果和标准答案的差距，损失函数越小，说明模型越稳健。我们的目标即让损失函数得到的值尽量的小。

这里结合`PyTorch`一起讲解一些常用损失函数。不会`PyTorch`的忽略就好了。

#### 平方损失函数

$$
Loss=\sum_{i=1}^n\frac{(y_i-label_i)^2}{n}
$$

```python
lossFunc = torch.nn.MSELoss()
...
loss = lossFunc(y, labels)
```

#### 交叉熵损失函数

**香农熵**我们知道是：
$$
entropy(x)=-\int_x p(x)\ln p(x)dx=-p(x)\ln p(x)-(1-p)\ln (1-p(x))
$$

而**KL散度**（相对熵）是：
$$
D_{KL}(P||Q)=\mathbb E_{\rm x\sim P}[\ln\frac{P(x)}{Q(x)}]=\sum P(x_i)\ln\frac{P(x_i)}{Q(x_i)}
$$
而**交叉熵**就是：
$$
H(P,Q)=D_{KL}(P||Q)-\sum P(x_i)\ln P(x_i)=-\sum P(x_i)\ln Q(x_i)
$$
（一般来说P对应的是标准答案，而Q对应是模型输出的结果）

> 这里多啰嗦两句，帮助理解：
>
> **熵**是用来描述不确定性（或者说混乱程度），可以用一个事件的信息量表示。而**KL散度**（相对熵）体现的是两件事情的差别大小（从一件事情看另外一件事情）。

若P(x)是实际的概率分布，我们的目标就是让Q(x)尽量逼近P(x)的分布，即使KL散度（相对熵）尽量接近0（因为KL散度是大于等于0的，所以让其尽量小就好）。而概率分布P(x)既然是真实的，那么$\sum P(x_i)\ln P(x_i)$就是固定的，那么让KL散度尽量小，就是让交叉熵尽量小。这么一转换我们也减少了计算量。其实MSELoss，在训练集的规模固定的情况下，我们也可以不去除那个$n$，减少计算量嘛~

```python
lossFunc = torch.nn.CrossEntropyLoss()
...
loss = lossFunc(y, labels)
```

>**选择**：
>
>* 如果使用sigmoid激活函数，则交叉熵损失函数一般肯定比均方差损失函数好；
>* 如果是DNN用于分类，则一般在输出层使用softmax激活函数和对数似然损失函数；
>
>上面两点摘自某CSDN博客，忘记co地址了，找到再补（汗颜）。

> **平方损失函数和交叉熵损失函数**
>
> 一般来说，**平方损失函数**适合对付和连续型变量相关的**回归问题**中。而**交叉熵损失函数**适合对付和离散Ont-Hot向量相关的**分类问题**。
> 	
> 在分类问题上，<u>平方损失函数对每一个输出结果都看重，而交叉熵损失函数只看重正确的结果</u>。
> 	
>
> 假设我们有一个三分类的问题，一个样本的标准答案（各类的概率）是(1,0,0)，而模型输出的结果是(a,b,c)。
> $$
> SquareLoss = [(a-1)^2+(b-0)^2+(c-0)^2]/3\\
> CrossEntropy = -1\times\ln a-0\times\ln b-0\times\ln c=-\ln a
> $$
> 可见SquareLoss和各结果都有关系，而CrossEntropy只和准确答案那项有关，所以我们发现，SquareLoss的意图是提高正确率（a的值），并且降低错误率（即逼着a往1方向发展，逼着b,c往0方向发展），而CrossEntropy只有一个目标提高正确率，而在分类问题下，各项概率之和为1，所以提高正确率，其他错误项的分类的概率自动就降了，在实际应用中也发现在**分类**问题下，交叉熵损失函数的性能更强。同样我们这么想，在回归问题上，如果我们用交叉熵函数得到的结果可能会不理想，假设一个三维空间，对同一个x，对应正确的坐标值为(x,0,0)而模型结果为(x,1,0.5)，那么：
> $$
> SquareLoss = \frac{(1-0)^2+(0.5-0)^2}{2}\\
> CrossEntropy = -0\times\ln 1-0\times\ln 0.5
> $$
> 可见，对于回归问题，CrossEntropy的结果没太多道理，相反，平方损失函数就有道理多了。

